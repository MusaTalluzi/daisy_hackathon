{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_data\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import pandas as pdj\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n"
     ]
    }
   ],
   "source": [
    "# Categorical base columns:\n",
    "date_ = tf.contrib.layers.sparse_column_with_hash_bucket(\"date_\", hash_bucket_size=356)\n",
    "store = tf.contrib.layers.sparse_column_with_hash_bucket(\"store\", hash_bucket_size=1000)\n",
    "department = tf.contrib.layers.sparse_column_with_hash_bucket(\"department\", hash_bucket_size=1000)\n",
    "item = tf.contrib.layers.sparse_column_with_hash_bucket(\"item\", hash_bucket_size=10000)\n",
    "on_promotion = tf.contrib.layers.sparse_column_with_keys(column_name=\"on_promotion\", keys=['0', '1'])\n",
    "promotion_type = tf.contrib.layers.sparse_column_with_hash_bucket(\"promotion_type\", hash_bucket_size=100)\n",
    "\n",
    "# Continuous base colunms\n",
    "unit_price = tf.contrib.layers.real_valued_column(\"unit_price\")\n",
    "\n",
    "# Wide and Deep columns:\n",
    "wide_columns = [\n",
    "    date_, store, department, item, on_promotion, promotion_type,\n",
    "    tf.contrib.layers.crossed_column([store, department], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([department, item], hash_bucket_size=int(1e6))\n",
    "]\n",
    "deep_columns = [\n",
    "    tf.contrib.layers.embedding_column(date_, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(store, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(department, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(item, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(on_promotion, dimension=8),\n",
    "    tf.contrib.layers.embedding_column(promotion_type, dimension=8),\n",
    "    unit_price\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_master': '', '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fefcd26aa90>, '_num_worker_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_is_chief': True, '_task_id': 0, '_environment': 'local', '_model_dir': '/tmp/tmp_x2bg_z4', '_evaluation_master': '', '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50],\n",
    "    fix_global_step_increment_bug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1352325 entries, 0 to 451254\n",
      "Data columns (total 8 columns):\n",
      "date_             1352325 non-null object\n",
      "store             1352325 non-null object\n",
      "department        1352325 non-null object\n",
      "item              1352325 non-null object\n",
      "unit_price        1352325 non-null float64\n",
      "quantity          1352325 non-null float64\n",
      "on_promotion      1352325 non-null object\n",
      "promotion_type    1352325 non-null object\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 92.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Column names:\n",
    "COLUMNS = [\"date_\", \"store\", \"department\", \"item\",\n",
    "           \"unit_price\", \"on_promotion\", \"promotion_type\"]\n",
    "LABEL_COLUMN = 'quantity'\n",
    "CATEGORICAL_COLUMNS = [\"date_\", \"store\", \"department\",\n",
    "                       \"item\", \"on_promotion\", \"promotion_type\"]\n",
    "CONTINUOUS_COLUMNS = [\"unit_price\"]\n",
    "\n",
    "# CATEGORICAL_COLUMNS = [\"item\"]\n",
    "# CONTINUOUS_COLUMNS = [\"unit_price\"]\n",
    "\n",
    "# Get data and split it into training and test:\n",
    "#data = pd.read_csv(\"./hackathon_data/test_data.csv\")\n",
    "\n",
    "data = import_data.import_data(file_regex=\"./hackathon_data/*20*.dat\")\n",
    "\n",
    "data[CATEGORICAL_COLUMNS] = data[CATEGORICAL_COLUMNS].astype(str)\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "df_train, df_test = model_selection.train_test_split(\n",
    "    data, test_size=0.2, random_state=42)\n",
    "# df_train[CATEGORICAL_COLUMNS] = df_train[CATEGORICAL_COLUMNS].astype(str)\n",
    "# df_test[CATEGORICAL_COLUMNS] = df_test[CATEGORICAL_COLUMNS].astype(str)\n",
    "# x = data[COLUMNS]\n",
    "# y = data[LABEL_COLUMN]\n",
    "\n",
    "# x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Scale training data to 0 mean and unit sd\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "#x_train = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary mappings between columns and their corresponding values:\n",
    "def input_fn(df):\n",
    "#     continuous_cols = {k: tf.constant(df[k].values)\n",
    "#                       for k in CONTINUOUS_COLUMNS}\n",
    "#     categorical_cols = {k: tf.SparseTensor(\n",
    "#       indices=[[i, 0] for i in range(df[k].size)],\n",
    "#       values=df[k].values,\n",
    "#       dense_shape=[df[k].size, 1])\n",
    "#                       for k in CATEGORICAL_COLUMNS}\n",
    "    \n",
    "    continuous_cols = {k: tf.constant(df[k].values,\n",
    "                                     shape=[df[k].size, 1])\n",
    "                      for k in CONTINUOUS_COLUMNS}\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      dense_shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "    \n",
    "    # Merge the dictionaries:\n",
    "    feature_cols = dict(continuous_cols.items() | categorical_cols.items())\n",
    "    label = tf.constant(df[LABEL_COLUMN].values) # label column into a const Tensor\n",
    "    return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /home/musa/.local/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2333: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:Casting <dtype: 'float64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float64'> labels to bool.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp_x2bg_z4/model.ckpt.\n",
      "INFO:tensorflow:loss = -3.8317518, step = 1\n"
     ]
    }
   ],
   "source": [
    "m.fit(input_fn=train_input_fn, steps=200)\n",
    "result = m.evaluate(input_fn=eval_input_fn, steps=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
